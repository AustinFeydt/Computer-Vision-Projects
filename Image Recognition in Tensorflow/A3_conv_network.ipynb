{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Austin Feydt (apf31)\n",
    "\n",
    "March 30 2018\n",
    "\n",
    "EECS 531: Assignment 3\n",
    "\n",
    "# Convolutional (LeNet-ish) Network\n",
    "\n",
    "To improve from the poor accuracy of our baseline network, I decided to emulate the original LeNet structure, with some tweaks along the way.  This network performs a convolution of 32 5x5 kernels, followed by 2x2 maxpooling to reduce the size of the image to 32x32. It then performs a convolution of 64 3x3 kernels, and another 2x2 maxpooling to further reduce to 16x16.  Then, there is a third convolution of 96 3x3 kernels, and yet again a 2x2 maxpooling reducing to 8x8 images. Then, all 8x8x96 images are strung out into one massive 6,144 inputs to 2 fully connected layers, reducing from 6,144 to 128, then to 64, then finally to 3 (all using rectified linear units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "TRAINING_SET_SIZE = 13718\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "class _image_object:\n",
    "    def __init__(self):\n",
    "        self.image = tf.Variable([], dtype = tf.string)\n",
    "        self.height = tf.Variable([], dtype = tf.int64)\n",
    "        self.width = tf.Variable([], dtype = tf.int64)\n",
    "        self.filename = tf.Variable([], dtype = tf.string)\n",
    "        self.label = tf.Variable([], dtype = tf.int32)\n",
    "\n",
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example, features = {\n",
    "        \"image/encoded\": tf.FixedLenFeature([], tf.string),\n",
    "        \"image/height\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/width\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/filename\": tf.FixedLenFeature([], tf.string),\n",
    "        \"image/class/label\": tf.FixedLenFeature([], tf.int64),})\n",
    "    image_encoded = features[\"image/encoded\"]\n",
    "    image_raw = tf.image.decode_jpeg(image_encoded, channels=CHANNELS)\n",
    "    print(image_raw)\n",
    "    image_object = _image_object()\n",
    "    image_object.image = tf.image.resize_image_with_crop_or_pad(image_raw, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    image_object.height = features[\"image/height\"]\n",
    "    image_object.width = features[\"image/width\"]\n",
    "    image_object.filename = features[\"image/filename\"]\n",
    "    image_object.label = tf.cast(features[\"image/class/label\"], tf.int64)\n",
    "    return image_object\n",
    "\n",
    "def net_input(if_random = True, if_training = True):\n",
    "    if(if_training):\n",
    "        filenames = [os.path.join(DATA_DIR, \"train-0000%d-of-00002.tfrecord\" % i) for i in range(0, 1)]\n",
    "    else:\n",
    "        filenames = [os.path.join(DATA_DIR, \"validation-0000%d-of-00002.tfrecord\" % i) for i in range(0, 1)]\n",
    "\n",
    "    for f in filenames:\n",
    "        if not tf.gfile.Exists(f):\n",
    "            raise ValueError(\"Failed to find file: \" + f)\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    image_object = read_and_decode(filename_queue)\n",
    "    image = tf.image.per_image_standardization(image_object.image)\n",
    "    label = image_object.label\n",
    "    filename = image_object.filename\n",
    "\n",
    "    image_batch, label_batch, filename_batch = tf.train.batch([image, label, filename],batch_size = BATCH_SIZE,num_threads = 1)\n",
    "    return image_batch, label_batch, filename_batch\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(val,shape):\n",
    "    if val == 0:\n",
    "        return tf.Variable(tf.zeros(shape))\n",
    "    else:\n",
    "        return tf.Variable(tf.constant(val, shape=shape))\n",
    "\n",
    "def conv2d(x, W, padding):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=padding)\n",
    "\n",
    "def max_pool_2x2(x, kernel_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet_inference(image_batch):\n",
    "    batch_size = 16\n",
    "    patch_size = 5\n",
    "    kernel_size_1 = 5\n",
    "    kernel_size_2 = 3\n",
    "    depth1 = 32 #the depth of 1st convnet\n",
    "    depth2 = 64 #the depth of 2nd convnet\n",
    "    depth3 = 96 #the depth of 3rd convnet\n",
    "    C5_units = 128\n",
    "    F6_units = 64\n",
    "    F7_units = 3\n",
    "\n",
    "    x_image = tf.reshape(image_batch, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS])\n",
    "    \n",
    "    print(x_image.shape)\n",
    "\n",
    "    # Start by defining all variables\n",
    "    W_conv1 = weight_variable([kernel_size_1, kernel_size_1, CHANNELS, depth1])\n",
    "    b_conv1 = bias_variable(0, [depth1])\n",
    "\n",
    "    #S2 has no weights\n",
    "\n",
    "    W_conv3 = weight_variable([kernel_size_2, kernel_size_2, depth1, depth2])\n",
    "    b_conv3 = bias_variable(1.0, [depth2])\n",
    "    \n",
    "    #S4 has no weights\n",
    "\n",
    "    W_conv5 = weight_variable([kernel_size_2, kernel_size_2, depth2, depth3])\n",
    "    b_conv5 = bias_variable(1.0, [depth3])\n",
    "\n",
    "    #S6 has no weights\n",
    "\n",
    "    W_conv7 = weight_variable([8*8*depth3, C5_units])\n",
    "    b_conv7 = bias_variable(1.0, [C5_units])\n",
    "    \n",
    "    W_fc8 = weight_variable([C5_units, F6_units])\n",
    "    b_fc8 = bias_variable(1.0, [F6_units])\n",
    "\n",
    "    W_fc9 = weight_variable([F6_units, F7_units])\n",
    "    b_fc9 = bias_variable(1.0, [F7_units])\n",
    "\n",
    "\n",
    "    #Convolutions and max pooling\n",
    "    conv = conv2d(x_image, W_conv1, 'SAME')\n",
    "    hidden = tf.nn.relu(conv + b_conv1)\n",
    "\n",
    "    max_pool = max_pool_2x2(hidden, kernel_size_1)\n",
    "    hidden = tf.nn.relu(max_pool)\n",
    "\n",
    "    conv = conv2d(hidden, W_conv3, 'SAME')\n",
    "    hidden = tf.nn.relu(conv + b_conv3)\n",
    "\n",
    "    max_pool = max_pool_2x2(hidden, kernel_size_2)\n",
    "    hidden = tf.nn.relu(max_pool)\n",
    "\n",
    "    conv = conv2d(hidden, W_conv5, 'SAME')\n",
    "    hidden = tf.nn.relu(conv + b_conv5)\n",
    "\n",
    "    max_pool = max_pool_2x2(hidden, kernel_size_2)\n",
    "    hidden = tf.nn.relu(max_pool)\n",
    "\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, W_conv7) + b_conv7)\n",
    "\n",
    "    #Fully connected layers leading to logits\n",
    "    fc = tf.matmul(hidden,W_fc8)\n",
    "    hidden = tf.nn.relu(fc + b_fc8)\n",
    "        \n",
    "    fc = tf.matmul(hidden,W_fc9)\n",
    "    output = fc + b_fc9\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def conv_train():\n",
    "    image_batch_out, label_batch_out, filename_batch = net_input(if_random = False, if_training = True)\n",
    "\n",
    "    image_batch_placeholder = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS])\n",
    "    image_batch = tf.reshape(image_batch_out, (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "\n",
    "    label_batch_placeholder = tf.placeholder(tf.float32, shape=[BATCH_SIZE, 3])\n",
    "    label_offset = -tf.ones([BATCH_SIZE], dtype=tf.int64, name=\"label_batch_offset\")\n",
    "    label_batch_one_hot = tf.one_hot(tf.add(label_batch_out, label_offset), depth=3, on_value=1.0, off_value=0.0)\n",
    "\n",
    "\n",
    "    logits_out = convnet_inference(image_batch_placeholder)\n",
    "    loss = tf.losses.mean_squared_error(labels=label_batch_placeholder, predictions=logits_out)\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        file_writer = tf.summary.FileWriter(\"./logs\", sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, \"output/checkpoint-train\")\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess = sess)\n",
    "\n",
    "        for i in range(10000):\n",
    "            image_out, label_out, label_batch_one_hot_out, filename_out = sess.run([image_batch, label_batch_out, label_batch_one_hot, filename_batch])\n",
    "            _, infer_out, loss_out = sess.run([train_step, logits_out, loss], feed_dict={image_batch_placeholder: image_out, label_batch_placeholder: label_batch_one_hot_out})\n",
    "          \n",
    "            if(i%25 == 0):\n",
    "                print(image_out.shape)\n",
    "                print(\"label_out: \")\n",
    "                print(filename_out)\n",
    "                print(label_out)\n",
    "                print(label_batch_one_hot_out)\n",
    "                print(\"infer_out: \")\n",
    "                print(infer_out)\n",
    "                saver.save(sess, \"output/checkpoint-train\")\n",
    "                print(\"loss: \")\n",
    "                print(loss_out)\n",
    "\n",
    "            if(loss_out < 0.02):\n",
    "            \tbreak\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "\n",
    "#conv_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"DecodeJpeg:0\", shape=(?, ?, 3), dtype=uint8)\n",
      "(16, 64, 64, 3)\n",
      "WARNING:tensorflow:From <ipython-input-3-aa51bd2a5c15>:13: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "INFO:tensorflow:Restoring parameters from output/checkpoint-train\n",
      "real labels:\n",
      "[2 0 0 0 2 2 2 2 1 2 1 2 0 1 0 2]\n",
      "predicted labels:\n",
      "[2 0 0 0 2 2 2 2 1 2 1 2 0 1 0 2]\n",
      "\n",
      "real labels:\n",
      "[0 1 0 2 2 2 1 0 2 0 1 0 1 0 1 2]\n",
      "predicted labels:\n",
      "[0 1 0 2 2 2 1 0 2 0 1 0 1 0 1 2]\n",
      "\n",
      "real labels:\n",
      "[1 0 0 0 0 2 1 0 0 0 2 2 2 0 1 2]\n",
      "predicted labels:\n",
      "[1 0 0 0 0 2 1 0 0 0 2 2 2 0 1 2]\n",
      "\n",
      "real labels:\n",
      "[1 1 2 1 2 1 0 0 0 2 1 2 2 2 0 1]\n",
      "predicted labels:\n",
      "[1 1 2 1 2 1 0 0 0 2 2 2 2 2 0 1]\n",
      "\n",
      "real labels:\n",
      "[1 0 0 1 0 2 1 2 0 1 1 1 0 2 2 2]\n",
      "predicted labels:\n",
      "[1 0 0 2 0 2 1 2 0 1 1 1 0 2 2 2]\n",
      "\n",
      "Accuracy: \n",
      "0.9875\n"
     ]
    }
   ],
   "source": [
    "def conv_evaluate():\n",
    "    image_batch_out, label_batch_out, filename_batch = net_input(if_random = False, if_training = False)\n",
    "\n",
    "    image_batch_placeholder = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS])\n",
    "    image_batch = tf.reshape(image_batch_out, (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "\n",
    "    label_tensor_placeholder = tf.placeholder(tf.int64, shape=[BATCH_SIZE])\n",
    "    label_offset = -tf.ones([BATCH_SIZE], dtype=tf.int64, name=\"label_batch_offset\")\n",
    "    label_batch = tf.add(label_batch_out, label_offset)\n",
    "\n",
    "    logits_out = tf.reshape(convnet_inference(image_batch_placeholder), [BATCH_SIZE, 3])\n",
    "    logits_batch = tf.to_int64(tf.arg_max(logits_out, dimension = 1))\n",
    "\n",
    "    correct_prediction = tf.equal(logits_batch, label_tensor_placeholder)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess,  \"output/checkpoint-train\")\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess = sess)\n",
    "\n",
    "        accuracy_accu = 0\n",
    "\n",
    "        for i in range(50):\n",
    "            image_out, label_out, filename_out = sess.run([image_batch, label_batch, filename_batch])\n",
    "\n",
    "            accuracy_out, logits_batch_out = sess.run([accuracy, logits_batch], feed_dict={image_batch_placeholder: image_out, label_tensor_placeholder: label_out})\n",
    "            accuracy_accu += accuracy_out\n",
    "            if i % 10 == 0: \n",
    "                print('real labels:')\n",
    "                print(label_out)\n",
    "                print('predicted labels:')\n",
    "                print(logits_batch_out)\n",
    "                print('')\n",
    "                \n",
    "        print(\"Accuracy: \")\n",
    "        print(accuracy_accu / 50)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "\n",
    "conv_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "As we can see, the accuracy of this new network is nearly perfect, with an accuracy of almost 99%.  The early layers of the convolutions are able to detect very specific, small features in the images.  After each 2x2 maxpooling, the features we discover are much more general/holistic.  This is precisely why convolutional neural networks work so well with images, compared to just fully connected networks (like our baseline classifier).  Obviously, we had to sacrifice a drastic increase in the number of hyperparameters for our network in order to achieve this much better network, but there are many current networks being developed that have been able to achieve near-perfect accuracy and also reduce hyperparameter sizes by up to 80% using advanced optimization techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
