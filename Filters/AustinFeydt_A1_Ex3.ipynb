{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Austin Feydt (apf31)\n",
    "\n",
    "EECS 531 - A1\n",
    "\n",
    "21 February 2018\n",
    "# Exercise 3: Template Matching\n",
    "\n",
    "# Mathematical Background:\n",
    "\n",
    "One strategy for identifying features in an image is to attempt to match a template to the image.  Similarly to a convolutional kernel, we will slide the template over the entire image.  However, rather than performing a convolution, we will instead define a metric to identify how \"different\" the template is to the current patch.\n",
    "\n",
    "Let A be our template and B be a patch of the image (of size $nxn$)\n",
    "$$A=[a_{ij}],\\quad B=[b_{ij}],\\quad 1\\leq i,j\\leq n$$\n",
    "\n",
    "Then, our distance function is defined as:\n",
    "$$D(A,B)=\\sum_{i=1}^{n}\\sum_{j=1}^{n}(a_{ij}-b_{ij})^2$$\n",
    "\n",
    "When $A=B$, then $D(A,B)=0$. Otherwise, $D(A,B)>0$, meaning that any non-zero result for $D(A,B)$ implies that there is some difference between the current patch and the template, whether it be noise or just true negative. Thus, we will compute this distance function over every patch in the image, and then classify the patch as a positive or a negative based on some threshold level of acceptable value for the distance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Letter Detection:\n",
    "\n",
    "I started by recreating the original letter detection problem, in which we try to identify 'h's in a block of typed text:\n",
    "&nbsp;\n",
    "Paragraph:\n",
    "![paragraph:](A1_Images/paragraph.png)\n",
    "\n",
    "&nbsp;\n",
    "Template:\n",
    "![template:](A1_Images/h_template.png)\n",
    "\n",
    "\n",
    "\n",
    "# Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, color\n",
    "from skimage.data import camera\n",
    "from skimage.filters import sobel\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates \"distance\" between pixel values for each patch in the image\n",
    "def get_distances(image, template):\n",
    "   #Get appropriate dimensions\n",
    "    par_height = image.shape[0]\n",
    "    par_width = image.shape[1]\n",
    "    template_height = template.shape[0]\n",
    "    template_width = template.shape[1]\n",
    "    rh = par_height - template_height\n",
    "    rw = par_width - template_width \n",
    "     \n",
    "    distances = np.zeros((rh,rw))\n",
    "\n",
    "    #scan over the image\n",
    "    for x in range(0,rh):\n",
    "        for y in range(0,rw):\n",
    "            patch = image[x:x + template_height,y: y + template_width]\n",
    "            # compute using distance function\n",
    "            distances[x,y] = np.sum(np.power(patch - template,2))\n",
    "    return distances\n",
    "\n",
    "# Draws bounding boxes around all positively classified objects (based on threshold)\n",
    "#and returns positive count needed for ROC\n",
    "def draw_boxes(image, template, distances, threshold, saveImage):\n",
    "    new_image = np.copy(image)\n",
    "    par_height = image.shape[0]\n",
    "    par_width = image.shape[1]\n",
    "    template_height = template.shape[0]\n",
    "    template_width = template.shape[1]\n",
    "    rh = par_height - template_height\n",
    "    rw = par_width - template_width \n",
    "     \n",
    "    pos_count = 0\n",
    "    total = 0\n",
    "    for x in range(0,rh):\n",
    "        for y in range(0,rw):\n",
    "            total += 1\n",
    "            if distances[x,y] < threshold:\n",
    "                pos_count+=1\n",
    "                new_image[x,y] = [1,0,0]\n",
    "                new_image[x,y:y + template_width] = [1,0,0]\n",
    "                new_image[x + template_height,y:y + template_width] = [1,0,0]\n",
    "                new_image[x: x + template_height,y] = [1,0,0]\n",
    "                new_image[x: x + template_height,y + template_width] = [1,0,0]\n",
    "    \n",
    "    # So we don't save a bunch of pictures in exercise 4 :D\n",
    "    if(saveImage):          \n",
    "        io.imsave(\"A1_Images/threshold\" + str(threshold) + \".png\", new_image)\n",
    "    return pos_count, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2328, 372360)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = io.imread(\"A1_Images/paragraph.png\")\n",
    "text = color.rgb2gray(text)\n",
    "\n",
    "template = io.imread(\"A1_Images/h_template.png\") \n",
    "template = color.rgb2gray(template)\n",
    "\n",
    "distances = get_distances(text,template)\n",
    "new_image = color.gray2rgb(text)\n",
    "\n",
    "draw_boxes(new_image, template, distances, 5, True)\n",
    "draw_boxes(new_image, template, distances, 20, True)\n",
    "draw_boxes(new_image, template, distances, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "&nbsp;\n",
    "Threshold=5\n",
    "![Threshold=5](A1_Images/threshold5.png)\n",
    "\n",
    "&nbsp;\n",
    "Threshold=20\n",
    "![Threshold=20](A1_Images/threshold20.png)\n",
    "\n",
    "&nbsp;\n",
    "Threshold=30\n",
    "![Threshold=30](A1_Images/threshold30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I have displayed a few images, showing what the classifier identified as positive at different distance-threshold values.  As we increase our threshold, we allow the classifier to make more mistakes, as it starts thinking that n's and m's are the same as h's, since they have similar shapes in their bottom halfs.  If we raise the threshold even more, we see that almost all of the letters are getting classified as positives (almost all of them are false positives!)  Although this example seems trivial, we will see in a later example how important it is to tune this threshold parameter, especially if we are interested in getting as many true positives as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A New Feature Detector (Shape Detection):\n",
    "\n",
    "To explore template matching more, I decided to make my own image.  It's comprised of rectangles, circles, and a few diamonds/polygons.  The goal for the classifier is to correctly identify the rectangles.  However, a few of the rectangles are obstructed by the other shapes.  We want to find an optimal threshold value that will identify these obstructed rectangles as positive examples!\n",
    "\n",
    "&nbsp;\n",
    "Original Image:\n",
    "![Image:](A1_Images/shapes.png)\n",
    "\n",
    "&nbsp;\n",
    "Template:\n",
    "![Template](A1_Images/box_template.png)\n",
    "\n",
    "\n",
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8064)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = io.imread(\"A1_Images/shapes.png\")\n",
    "text = color.rgb2gray(text)\n",
    "\n",
    "template = io.imread(\"A1_Images/box_template.png\") \n",
    "template = color.rgb2gray(template)\n",
    "\n",
    "distances = get_distances(text,template)\n",
    "new_image = color.gray2rgb(text)\n",
    "\n",
    "draw_boxes(new_image, template, distances, 1, True)\n",
    "draw_boxes(new_image, template, distances, 9, True)\n",
    "draw_boxes(new_image, template, distances, 15, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Results:\n",
    "&nbsp;\n",
    "Different Threshold Results:\n",
    "![different threshold results:](A1_Images/result_shape.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this example, we were interested in finding a threshold value that would correctly classify the obstructed rectangles.  By starting the threshold at a low value, we can see that it can correctly identify all un-obstructed rectangles.  However, as we increase the threshold, we see that the obstructed rectangles begin to fall below the threshold, and they get correctly classified. We can see that the more obstructed the object, the higher the threshold must be to correctly classify.  However, this begs the question: do we actually want to correctly classify an object that's 25% or 50% covered by another image? What is our confidence that this obstructed object is even what we think it is? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
